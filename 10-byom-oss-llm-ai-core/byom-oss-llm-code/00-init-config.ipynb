{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial configuration of SAP AI Core for BYOM-OSS-LLM-AI-CORE\n",
    "This notebook automates the initial configurations for application BYOM-OSS-LLM-AI-CORE to bring open-sourced llms into SAP AI Core. Alternatively, you can perform the same with SAP AI Launchpad.\n",
    "- Onboarding Git Repository\n",
    "- Create an Application and Synchronize\n",
    "- Create the configurations for scenarios ollama, local-ai, llama.cpp, and vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0: Replace <YOUR_DOCKER_SECRET> and <YOUR_DOCKER_USER> in serving templates\n",
    "First of most, replace the place holders <YOUR_DOCKER_SECRET> and <YOUR_DOCKER_USER> with your docker secret and user in the following serving templates.\n",
    "```yaml\n",
    "    spec: |\n",
    "      predictor:\n",
    "        imagePullSecrets:\n",
    "          - name: <YOUR_DOCKER_SECRET>\n",
    "          ...\n",
    "        containers:\n",
    "            - name: kserve-container\n",
    "              image: docker.io/<YOUR_DOCKER_USER>/ollama:ai-core\n",
    "```\n",
    "- [../byom-oss-llm-templates/llama.cpp-template.yaml](../byom-oss-llm-templates/llama.cpp-template.yaml)\n",
    "- [../byom-oss-llm-templates/local-ai-template.yaml](../byom-oss-llm-templates/local-ai-template.yaml)\n",
    "- [../byom-oss-llm-templates/ollama-template.yaml](../byom-oss-llm-templates/ollama-template.yaml)\n",
    "- [../byom-oss-llm-templates/vllm-template.yaml](../byom-oss-llm-templates/vllm-template.yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Copy [config.template.json](config.template.jso) as [config.json](config.json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cp config.template.json config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Review and Update configuration in [config.json](config.json)\n",
    "Please read the **comments** carefully in [config.json](config.json) and update the necessary configurations.  \n",
    "- **name**: used as name of git repository and application. \n",
    "- **resource_group**: \"default\" will be used if not specified. It is optional but recommended to create a dedicate resource group, and update it [config.json](config.json). By default, \"default\" resource group is in place for all the AI Core instances.AI Core with tree tier plan is not able to create a new resource group.\n",
    "- **ai_core_sk**: update with your own AI Core Service Key\n",
    "- **git_repo**: update the git repo configuration with your owns\n",
    "    - repo_url: url to your forked repository. It should be: https://github.com/<YOUR_GITHUB_ORG_OR_USER>/btp-generative-ai-hub-use-cases\n",
    "    - user: update with your github user\n",
    "    - access_token: update with your github user access token\n",
    "- **application**: The SAP AI Core application hosts the scenarios of ollama etc to serving open sourced llms in SAP AI Core\n",
    "    - path_in_repo: relative path to the serving templates. No change needed.\n",
    "- **configurations**: Review the configurations of the scenarios. By default, it is configured to load the mistral-7b quantization model with [resource plan infer.s in SAP AI Core](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/choose-resource-plan-c58d4e584a5b40a2992265beb9b6be3c) defined in [../byom-oss-llm-templates](../byom-oss-llm-templates). It is recommend to go ahead first with the default configurations in config.json.\n",
    "    - **Ollama**: No configuration required for ollama. Pull the model dynamically in [ollama/ollama.ipynb](ollama/ollama.ipynb)\n",
    "    - **LocalAI**: LocalAI enable you to [preload model during startup](https://localai.io/advanced/#preloading-models-during-startup). The initial configuration in config.json will preload model [Mistral-7B-OpenOrca-GGUF](https://github.com/go-skynet/model-gallery/blob/main/mistral.yaml) with local-ai on resource plan 'infer.s' defined in [local-ai-template.yaml](../byom-oss-llm-templates/local-ai-template.yaml). In its model config file, GPU acceleration isn't enabled, hence it is quite slow. To have GPU acceleration for a model, you may set in its model config yaml file. For example [mixtral-Q6.yaml](https://github.com/go-skynet/model-gallery/blob/main/mixtral-Q6.yaml). Please review the [full config model file reference](https://localai.io/advanced/#full-config-model-file-reference)\n",
    "        ```sh\n",
    "        f16: true \n",
    "        mmap: true \n",
    "        gpu_layers: xx \n",
    "        ```\n",
    "        In addition, you can install more models through end point /model/apply in [local-ai/local-ai.ipynb](local-ai/local-ai.ipynb). Please refer to https://localai.io/advanced/#preloading-models-during-startup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Load the configurations from [config.json](config.json)\n",
    "The service key of AI Core are located in section ai_core_sk of [config.json](config.json).<br/>\n",
    "Please update it with your own service key before running this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations loaded from config.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initializations\n",
    "resource_group = config.get(\"resource_group\", \"default\")\n",
    "name = config.get(\"name\", \"open-source-llms\")\n",
    "print(\"Configurations loaded from config.json\")\n",
    "print(\"name: \", name, \"resource_group: \", resource_group )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: Initialize AI Core SDK Client\n",
    "The service key of AI Core are located in section ai_core_sk of [config.json](config.json).<br/>\n",
    "Please update it with your own service key before running this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource group: oss-llm, name: byom-open-source-llms\n"
     ]
    }
   ],
   "source": [
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "from ai_core_sdk.models import ParameterBinding\n",
    "\n",
    "ai_core_sk = config[\"ai_core_service_key\"]\n",
    "client = AICoreV2Client(base_url=ai_core_sk.get(\"serviceurls\").get(\"AI_API_URL\")+\"/v2\",\n",
    "                        auth_url=ai_core_sk.get(\"url\")+\"/oauth/token\",\n",
    "                        client_id=ai_core_sk.get(\"clientid\"),\n",
    "                        client_secret=ai_core_sk.get(\"clientsecret\"),\n",
    "                        resource_group=resource_group)\n",
    "print(f\"resource group: {resource_group}, name: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4: Create a dedicated resource group (Optional but recommended)\n",
    "resource_group defined here must be matched with resource_group in [config.json](config.json). Default as \"oss-llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_group = \"oss-llm\"\n",
    "\n",
    "response = client.resource_groups.create(resource_group_id = resource_group)\n",
    "print(response.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Create repository and application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Repository has been on-boarded.\n",
      "Id: byom-open-source-llms, Message: Application has been successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Onboard repository\n",
    "repo_config = config[\"git_repo\"]\n",
    "repository = client.repositories.create(name,\n",
    "                                        url=repo_config.get(\"repo_url\"),\n",
    "                                        username=repo_config.get(\"user\"),\n",
    "                                        password=repo_config.get(\"access_token\")\n",
    "                                        )\n",
    "print(repository)\n",
    "\n",
    "# Create application\n",
    "app_config = config[\"application\"]\n",
    "application = client.applications.create(revision=app_config.get(\"revision\", \"HEAD\"),\n",
    "                                        path=app_config.get(\"path_in_repo\"),\n",
    "                                        application_name=name,\n",
    "                                        repository_name=name\n",
    "                                        )\n",
    "print(application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Check if application has synced and scenario created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health Status: Healthy, Sync Status: Synced, Sync Finished at: 2024-03-20T05:11:45Z\n",
      "Application synced\n",
      "Scenario ollama synced\n",
      "Scenario local-ai synced\n",
      "Scenario llama.cpp synced\n",
      "Scenario vllm synced\n"
     ]
    }
   ],
   "source": [
    "max_tries = 10\n",
    "i = 0\n",
    "interval_s = 20\n",
    "while i < max_tries:\n",
    "    i = i +1\n",
    "    app_status = client.applications.get_status(name)\n",
    "    print(f\"Health Status: {app_status.health_status}, Sync Status: {app_status.sync_status}, Sync Finished at: {app_status.sync_finished_at}\" )\n",
    "    \n",
    "    if(app_status.sync_status == \"Synced\"):\n",
    "        break\n",
    "\n",
    "    # Synchronize the application and wait\n",
    "    client.applications.refresh(name) \n",
    "    sleep(interval_s)\n",
    "\n",
    "if app_status.sync_status == \"Synced\":\n",
    "    print(\"Application synced\")\n",
    "    # Check scenarios\n",
    "    scenarios = client.scenario.query()\n",
    "\n",
    "    scenario_list = config[\"scenarios\"]\n",
    "    for scenario in scenario_list:\n",
    "        scenario_name = scenario[\"name\"]\n",
    "        scenario_exists = scenario_name in [s.name for s in scenarios.resources]\n",
    "        print(f\"Scenario {scenario} synced\") if scenario_exists else print(f\"Scenario {scenario_name} not yet available\")\n",
    "\n",
    "else:\n",
    "    #print(f\"Application not yet synced after 10 time retry. Likely, something wrong in the templates under git repo {repository.url}/{app_config.get(\"path_in_repo\")}.\\nPlease check it. You can run this cell again once it is fixed.\")\n",
    "    print(f\"Application not yet synced after 10 time retry. Please execute this cell again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Create configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_json_file(file_path, key, value):\n",
    "    # Load the JSON configuration file\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "    # Update the value\n",
    "    config[key] = value\n",
    "\n",
    "    # Write the updated configuration back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(config, file, indent=4)\n",
    "        print(f\"{file_path} updated. {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------byom-ollama-server--------------\n",
      "Id: 61fb062e-4bdb-40d7-918b-ee48fac7d48b, Message: Configuration created\n",
      "ollama/env.json updated. configuration_id: 61fb062e-4bdb-40d7-918b-ee48fac7d48b\n",
      "--------------byom-local-ai-server--------------\n",
      "Id: c972d7d5-4357-4aa0-9671-a9bcd4469538, Message: Configuration created\n",
      "local-ai/env.json updated. configuration_id: c972d7d5-4357-4aa0-9671-a9bcd4469538\n",
      "--------------byom-llama.cpp-server--------------\n",
      "Id: 33a87ba9-48e1-431c-b963-282665428ca4, Message: Configuration created\n",
      "llama.cpp/env.json updated. configuration_id: 33a87ba9-48e1-431c-b963-282665428ca4\n",
      "--------------byom-vllm-server--------------\n",
      "Id: 6a640f56-df21-487e-855b-5e1cde04bd9c, Message: Configuration created\n",
      "vllm/env.json updated. configuration_id: 6a640f56-df21-487e-855b-5e1cde04bd9c\n"
     ]
    }
   ],
   "source": [
    "# Create serving configurations\n",
    "conf_list = config[\"configurations\"]\n",
    "\n",
    "for conf in conf_list:\n",
    "    parameter_bindings = [ParameterBinding(pb['key'], pb['value']) for pb in conf[\"parameters\"]]    \n",
    "    configuration = client.configuration.create(\n",
    "        name=conf[\"name\"],\n",
    "        scenario_id=conf[\"scenario_id\"],\n",
    "        executable_id=conf[\"executable_id\"],\n",
    "        parameter_bindings=parameter_bindings,\n",
    "    )\n",
    "    print(f'--------------{conf[\"scenario_id\"]}--------------')\n",
    "    print(configuration)\n",
    "\n",
    "    # Update the configuration_id in env.json under the corresponding folder\n",
    "    # which will be used in continuos-deployment.ipynb to create deployment automatically.\n",
    "    update_json_file(f'{conf[\"executable_id\"]}/env.json',\"configuration_id\", configuration.id)\n",
    "    config_id = configuration.id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
