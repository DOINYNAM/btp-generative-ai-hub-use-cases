{
  "comment": "name will be used as git repository name and application name. the value of ai_core_service_key section should be copied from service key of your AI Core instance",
  "name": "byom-open-source-llms",
  "resource_group": "oss-llm",
  "ai_core_service_key": {
    "clientid": "<REPLACE_WITH_YOUR_AI_CORE_CLIENT_ID>",
    "clientsecret": "<REPLACE_WITH_YOUR_AI_CORE_CLIENT_SECRET>",
    "url": "<REPLACE_WITH_YOUR_AI_CORE_URL> something like https://xxxxxx.authentication.eu10.hana.ondemand.com",
    "identityzone": "<REPLACE_WITH_YOUR_AI_CORE_ID_ZONE>",
    "identityzoneid": "<REPLACE_WITH_YOUR_AI_CORE_ID_ZONE_ID>",
    "appname": "<REPLACE_WITH_YOUR_AI_CORE_ID_ZONE_APPNAME>",
    "serviceurls": {
      "AI_API_URL": "<REPLACE_WITH_YOUR_AI_CORE_AI_API_URL> Something like https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com"
    }
  },
  "docker_secret": {
    "name": "docker-secret",
    "data": {
      ".dockerconfigjson": "{\"auths\": {\"docker.io\": {\"username\": \"<REPLACE_WITH_YOUR_DOCKER_USERNAME>\", \"password\": \"<REPLACE_WITH_YOUR_DOCKER_ACCESS_TOKEN>\"}}}"
    }
  },
  "git_repo": {
    "repo_url": "https://github.com/<YOUR_GIT_HUB_USER>/btp-generative-ai-hub-use-cases",
    "user": "<REPLACE_WITH_YOUR_GITHUB_USER>",
    "access_token": "<REPLACE_WITH_YOUR_GITHUB_USER_ACCESS_TOKEN>"
  },
  "application": {
    "path_in_repo": "10-byom-oss-llm-ai-core/byom-oss-llm-templates",
    "revision": "HEAD"
  },
  "comment_scenario": "Scenarios are defined in serving templates under byom-oss-llm-templates.DO NOT change anything in scenarios section!",
  "scenarios": [
    {
      "name": "ollama",
      "id": "ollama"
    },
    {
      "name": "local-ai",
      "id": "local-ai"
    },
    {
      "name": "llama.cpp",
      "id": "llama.cpp"
    },
    {
      "name": "vllm",
      "id": "vllm"
    }
  ],
  "comment_configurations": "DO NOT change scenario_id and executable_id in configurations section! You may change the name and parameters of each configuration",
  "configurations": [
    {
      "comment": "No configuration required for ollama. Pull the model dynamically in ollama/ollama.ipynb",
      "name": "ollama",
      "scenario_id": "ollama",
      "executable_id": "ollama",
      "parameters": []
    },
    {
      "comment": "Configuration below is to preload model Mistral-7B-OpenOrca-GGUF(https://github.com/go-skynet/model-gallery/blob/main/mistral.yaml) with local-ai on resource plan 'infer s' defined in local-ai-template.yaml. In its model config file, GPU acceleration isn't enabled, hence it is quite slow. To have GPU acceleration for a model, you may set f16: true, mmap:true and gpu_layers: xx as example https://github.com/go-skynet/model-gallery/blob/main/mixtral-Q6.yaml. You can install more models with end point /model/apply in local-ai/local-ai.ipynb. Please refer to https://localai.io/advanced/#preloading-models-during-startup",
      "name": "local-ai",
      "scenario_id": "local-ai",
      "executable_id": "local-ai",
      "parameters": [{
        "key": "preloaded_models",
        "value": "[{\"id\": \"model-gallery@mistral\"}]"
      },
      {
        "key": "debug",
        "value": "true"
      }]
    },
    {
      "comment": "Configuration is to run model mistral-7b-instruct-v0.2.Q5_K_M with llama.cpp on resource plan 'infer s' defined in llama.cpp-template.yaml. You may modify it with your target model. ngl stands for the number of layers to offload to GPU. Please refer to https://github.com/ggerganov/llama.cpp/tree/c47cf414efafb8f60596edc7edb5a2d68065e992/examples/server about arguments of llama.cpp server",
      "name": "llama.cpp",
      "scenario_id": "llama.cpp",
      "executable_id": "llama.cpp",
      "parameters": [{
        "key": "modelDownloadUrl",
        "value": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf"
      },
      {
        "key": "modelFileName",
        "value": "mistral-7b-instruct-v0.2.Q5_K_M.gguf"
      },
      {
        "key": "alias",
        "value": "mistral"
      },
      {
        "key": "threads",
        "value": "3"
      },
      {
        "key": "ctxSize",
        "value": "4098"
      },
      {
        "key": "ngl",
        "value": "30"
      },
      {
        "key": "enableEmbeddings",
        "value": "false"
      }]
    },
    {
      "comment": "Configuration is to run model Mistral-7B-Instruct-v0.2-AWQ with vllm on resource plan 'infer s' defined in vllm-template.yaml. You may modify it with your target model. Please refer to https://docs.vllm.ai/en/latest/models/engine_args.html for engine arguments detail",
      "name": "vllm",
      "scenario_id": "vllm",
      "executable_id": "vllm",
      "parameters": [{
        "key": "model",
        "value": "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
      },
      {
        "key": "dataType",
        "value": "half"
      },
      {
        "key": "gpuMemoryUtilization",
        "value": "0.95"
      },
      {
        "key": "maxTokenLen",
        "value": "2048"
      },
      {
        "key": "maxNumBatchedTokens",
        "value": "2048"
      },
      {
        "key": "maxNumSeqs",
        "value": "2048"
      },
      {
        "key": "quantization",
        "value": "awq"
      }]
    }
  ]
}
